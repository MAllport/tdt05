{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "## Team name: Team Hilbert Space\n",
    "### Michael Moen Allport & Jonas Sandberg\n",
    "### Student IDs: 747903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>...</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>254e988c9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8928</td>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3829ad8a4</td>\n",
       "      <td>...</td>\n",
       "      <td>79df04ed6</td>\n",
       "      <td>A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6d3db6c57</td>\n",
       "      <td>a7059911d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>e3282379d</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8098</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>fd943bc67</td>\n",
       "      <td>...</td>\n",
       "      <td>2815e62ea</td>\n",
       "      <td>U</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>abce980f5</td>\n",
       "      <td>e60e65034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>c362abead</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.7761</td>\n",
       "      <td>k</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>b770c0db2</td>\n",
       "      <td>...</td>\n",
       "      <td>8496c8e33</td>\n",
       "      <td>Q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>63dde2492</td>\n",
       "      <td>d3d8f55e0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>84ab3eece</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9302</td>\n",
       "      <td>i</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5968f0acd</td>\n",
       "      <td>...</td>\n",
       "      <td>b31517d23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cb0d7da76</td>\n",
       "      <td>3600c6e91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a16717598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0238</td>\n",
       "      <td>e</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>221405824</td>\n",
       "      <td>...</td>\n",
       "      <td>6327fb08b</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8ce034bb6</td>\n",
       "      <td>93360bbc9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target         f0   f1      f2 f3   f4   f5   f6         f7  ...  \\\n",
       "0   0       0  254e988c9  0.0  6.8928  a  0.0  0.2  0.3  3829ad8a4  ...   \n",
       "1   1       0  e3282379d  0.0  6.8098  n  NaN  0.0  0.1  fd943bc67  ...   \n",
       "2   2       0  c362abead  1.0  6.7761  k  0.0  0.1  0.0  b770c0db2  ...   \n",
       "3   3       0  84ab3eece  0.0  6.9302  i  3.0  0.2  0.3  5968f0acd  ...   \n",
       "4   4       0  a16717598  0.0  7.0238  e  4.0  0.1  0.4  221405824  ...   \n",
       "\n",
       "         f15  f16  f17   f18  f19  f20  f21  f22        f23        f24  \n",
       "0  79df04ed6    A  2.0  51.0  3.0  1.0    D  0.0  6d3db6c57  a7059911d  \n",
       "1  2815e62ea    U  0.0  10.0  1.0  3.0    E  0.0  abce980f5  e60e65034  \n",
       "2  8496c8e33    Q  1.0  16.0  3.0  0.0  NaN  0.0  63dde2492  d3d8f55e0  \n",
       "3  b31517d23  NaN  1.0  63.0  3.0  0.0    E  0.0  cb0d7da76  3600c6e91  \n",
       "4  6327fb08b    H  1.0  26.0  NaN  3.0    A  0.0  8ce034bb6  93360bbc9  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('datasets/challenge1_train.csv') as train_csv:\n",
    "        df = pd.read_csv(train_csv, skipinitialspace=True)\n",
    "labels = df['target']\n",
    "features = df.drop(columns=['target','id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have a variety of data types. Some columns are hex values, others are categorical, float or boolean values.\n",
    "Lets check out the amount of missing values to see if there any outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 25 artists>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "null_counts = features.isna().sum()/len(df)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.xticks(np.arange(len(null_counts)),null_counts.index)\n",
    "plt.ylabel('fraction of rows with missing data')\n",
    "plt.bar(np.arange(len(null_counts)),null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we do have a fair amount of missing values, 3% is manageable and there are no outliers between columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f8</th>\n",
       "      <th>f10</th>\n",
       "      <th>f11</th>\n",
       "      <th>f12</th>\n",
       "      <th>f13</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48492.000000</td>\n",
       "      <td>48594.000000</td>\n",
       "      <td>48424.000000</td>\n",
       "      <td>48473.000000</td>\n",
       "      <td>48552.000000</td>\n",
       "      <td>48463.000000</td>\n",
       "      <td>48500.00000</td>\n",
       "      <td>48553.000000</td>\n",
       "      <td>48491.00000</td>\n",
       "      <td>48471.000000</td>\n",
       "      <td>48509.000000</td>\n",
       "      <td>48501.000000</td>\n",
       "      <td>48478.000000</td>\n",
       "      <td>48475.000000</td>\n",
       "      <td>48530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.093356</td>\n",
       "      <td>6.988378</td>\n",
       "      <td>3.094148</td>\n",
       "      <td>0.146263</td>\n",
       "      <td>0.310677</td>\n",
       "      <td>6.350866</td>\n",
       "      <td>0.53732</td>\n",
       "      <td>0.276626</td>\n",
       "      <td>4.11532</td>\n",
       "      <td>0.037251</td>\n",
       "      <td>0.648003</td>\n",
       "      <td>62.147378</td>\n",
       "      <td>1.955196</td>\n",
       "      <td>1.889964</td>\n",
       "      <td>0.183721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.290933</td>\n",
       "      <td>0.152945</td>\n",
       "      <td>1.845653</td>\n",
       "      <td>0.065318</td>\n",
       "      <td>0.185038</td>\n",
       "      <td>3.455614</td>\n",
       "      <td>0.49861</td>\n",
       "      <td>0.447334</td>\n",
       "      <td>2.03611</td>\n",
       "      <td>0.048348</td>\n",
       "      <td>0.823160</td>\n",
       "      <td>33.086578</td>\n",
       "      <td>0.853246</td>\n",
       "      <td>1.529719</td>\n",
       "      <td>0.387261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.521800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.870225</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.987500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.102300</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.479400</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.00000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 f1            f2            f4            f5            f6  \\\n",
       "count  48492.000000  48594.000000  48424.000000  48473.000000  48552.000000   \n",
       "mean       0.093356      6.988378      3.094148      0.146263      0.310677   \n",
       "std        0.290933      0.152945      1.845653      0.065318      0.185038   \n",
       "min        0.000000      6.521800      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      6.870225      1.000000      0.100000      0.200000   \n",
       "50%        0.000000      6.987500      4.000000      0.200000      0.400000   \n",
       "75%        0.000000      7.102300      5.000000      0.200000      0.500000   \n",
       "max        1.000000      7.479400      5.000000      0.200000      0.500000   \n",
       "\n",
       "                 f8          f10           f11          f12           f13  \\\n",
       "count  48463.000000  48500.00000  48553.000000  48491.00000  48471.000000   \n",
       "mean       6.350866      0.53732      0.276626      4.11532      0.037251   \n",
       "std        3.455614      0.49861      0.447334      2.03611      0.048348   \n",
       "min        1.000000      0.00000      0.000000      1.00000      0.000000   \n",
       "25%        3.000000      0.00000      0.000000      2.00000      0.000000   \n",
       "50%        6.000000      1.00000      0.000000      5.00000      0.000000   \n",
       "75%        8.000000      1.00000      1.000000      6.00000      0.100000   \n",
       "max       12.000000      1.00000      1.000000      7.00000      0.100000   \n",
       "\n",
       "                f17           f18           f19           f20           f22  \n",
       "count  48509.000000  48501.000000  48478.000000  48475.000000  48530.000000  \n",
       "mean       0.648003     62.147378      1.955196      1.889964      0.183721  \n",
       "std        0.823160     33.086578      0.853246      1.529719      0.387261  \n",
       "min        0.000000      1.000000      1.000000      0.000000      0.000000  \n",
       "25%        0.000000     32.000000      1.000000      0.000000      0.000000  \n",
       "50%        0.000000     65.000000      2.000000      2.000000      0.000000  \n",
       "75%        1.000000     94.000000      3.000000      3.000000      0.000000  \n",
       "max        3.000000    104.000000      3.000000      4.000000      1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While pandas describe() is decent for gettting a brief overview of the dataset, it can sometimes be misleading. A standard deviation of 3.4 in column f8 might be worrying until one realizes that the column has 12 distinct values and is probably best represented ordinally.\n",
    "\n",
    "A more powerful tool is \"Pandas profiling\", and is far more useful for data exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#profile = ProfileReport(df.drop(columns=['id']))\n",
    "#profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot going on in this report, so let's break the different aspects into parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data types\n",
    "\n",
    "We essentially have five different data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features     = ['f2', 'f18']\n",
    "hexadecimal_features = ['f0', 'f7', 'f15', 'f23', 'f24']\n",
    "boolean_features     = ['f1', 'f10', 'f11', 'f13', 'f22']\n",
    "ordinal_features     = ['f4', 'f5', 'f6', 'f8', 'f12', 'f17', 'f19', 'f20']\n",
    "categorical_features = ['f3', 'f9', 'f14', 'f16', 'f21']\n",
    "\n",
    "features = features.drop(columns=['f9'])\n",
    "categorical_features.remove('f9')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 5 columns with hexadecimal numbers, so we need to convert them to a numeric type. It could also be argued that certain columns with hexadecimal numbers should be represented as categorical features if the number of distinct rows is low enough, for example column f23 and f24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced classes\n",
    "\n",
    "We see that the targets are heavily favored towards \"0\". This needs to be dealt with, so our model doesn't heavily lean towards guessing \"0\". It also impacts the metrics we use to evaluate our model, given that a model that always guesses \"0\" will have an 80% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicates\n",
    "We inspect the training set and find that we have no duplicates, so we don't need to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations\n",
    "\n",
    "We see that the only correlation relationships are very weak. This is good, because correlated columns can have adverse effects on certain classification algorithms, like Logistic Regression and Random Forest. However, the data is not processed, so the report might be misleading - hex columns are treated as categorical values, for example. Let's pre-process and quantify how strong the relationships are - to do that we need to use different algorithms for the numeric features and categorical features. We use Pearson's for numeric data and Pandas' crosstab() for categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make copies of the training set to analyze correlations\n",
    "features_corr = features.copy()\n",
    "labels_corr = labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from itertools import combinations\n",
    "from scipy.stats import chi2_contingency, pearsonr\n",
    "\n",
    "def prob(p, col1, col2,coef):\n",
    "    alpha = 0.05\n",
    "    if p < alpha:\n",
    "        print(\"Comparing \" + col1 + \" and \" + col2)\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)\n",
    "        print(\"Correlation coefficient is coef=%.3f\", coef)\n",
    "    \n",
    "def crosstab(df):\n",
    "    print(\"Analyzing relationship with Contingency table\")\n",
    "    comb = combinations(list(df), 2)\n",
    "    for col1, col2 in comb:\n",
    "        p = chi2_contingency(pd.crosstab(df[col1], df[col2]))[1]\n",
    "        prob(p, col1, col2)\n",
    "    \n",
    "def pearson(df):\n",
    "    print(\"Analyzing relationship with Pearson's\")\n",
    "    comb = combinations(list(df), 2)\n",
    "    for col1, col2 in comb:\n",
    "        coef, p = pearsonr(col1, col2)\n",
    "        # interpret the significance\n",
    "        prob(p, col1, col2, coef)\n",
    "\n",
    "    \n",
    "def analyze_corr(features, labels):\n",
    "    le = LabelEncoder()\n",
    "    for i in range(25):\n",
    "        feature = \"f\" + str(i)\n",
    "        if feature in hexadecimal_features:\n",
    "            features[feature] = features[feature].apply(lambda x: conv_hex(x))\n",
    "        if feature in categorical_features:\n",
    "            features[feature] = pd.get_dummies(features[feature])\n",
    "\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    features = pd.DataFrame(min_max_scaler.fit_transform(features), columns=features.columns, index=features.index)\n",
    "    df_num = features[features.columns & (numeric_features + hexadecimal_features)]\n",
    "    df_cat   = features[features.columns & (categorical_features + boolean_features + ordinal_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## Transform hexadecimal features to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f15</th>\n",
       "      <th>f16</th>\n",
       "      <th>f17</th>\n",
       "      <th>f18</th>\n",
       "      <th>f19</th>\n",
       "      <th>f20</th>\n",
       "      <th>f21</th>\n",
       "      <th>f22</th>\n",
       "      <th>f23</th>\n",
       "      <th>f24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.001453e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8928</td>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.507609e+10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.271454e+10</td>\n",
       "      <td>A</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>D</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.932418e+10</td>\n",
       "      <td>4.483459e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.097694e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.8098</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.806960e+10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.076038e+10</td>\n",
       "      <td>U</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.611909e+10</td>\n",
       "      <td>6.175525e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.244838e+10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.7761</td>\n",
       "      <td>k</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.924192e+10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.559159e+10</td>\n",
       "      <td>Q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.680777e+10</td>\n",
       "      <td>5.686738e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.561304e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9302</td>\n",
       "      <td>i</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.400079e+10</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.807206e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.450654e+10</td>\n",
       "      <td>1.449633e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.332621e+10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0238</td>\n",
       "      <td>e</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>9.147800e+09</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.661703e+10</td>\n",
       "      <td>H</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.781606e+10</td>\n",
       "      <td>3.951668e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             f0   f1      f2 f3   f4   f5   f6            f7   f8  f10  ...  \\\n",
       "0  1.001453e+10  0.0  6.8928  a  0.0  0.2  0.3  1.507609e+10  4.0  0.0  ...   \n",
       "1  6.097694e+10  0.0  6.8098  n  NaN  0.0  0.1  6.806960e+10  2.0  1.0  ...   \n",
       "2  5.244838e+10  1.0  6.7761  k  0.0  0.1  0.0  4.924192e+10  3.0  1.0  ...   \n",
       "3  3.561304e+10  0.0  6.9302  i  3.0  0.2  0.3  2.400079e+10  6.0  0.0  ...   \n",
       "4  4.332621e+10  0.0  7.0238  e  4.0  0.1  0.4  9.147800e+09  5.0  1.0  ...   \n",
       "\n",
       "            f15  f16  f17   f18  f19  f20  f21  f22           f23  \\\n",
       "0  3.271454e+10    A  2.0  51.0  3.0  1.0    D  0.0  2.932418e+10   \n",
       "1  1.076038e+10    U  0.0  10.0  1.0  3.0    E  0.0  4.611909e+10   \n",
       "2  3.559159e+10    Q  1.0  16.0  3.0  0.0  NaN  0.0  2.680777e+10   \n",
       "3  4.807206e+10  NaN  1.0  63.0  3.0  0.0    E  0.0  5.450654e+10   \n",
       "4  2.661703e+10    H  1.0  26.0  NaN  3.0    A  0.0  3.781606e+10   \n",
       "\n",
       "            f24  \n",
       "0  4.483459e+10  \n",
       "1  6.175525e+10  \n",
       "2  5.686738e+10  \n",
       "3  1.449633e+10  \n",
       "4  3.951668e+10  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_hex(x):\n",
    "    try:\n",
    "        return int(x, 16)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    except TypeError:\n",
    "        return np.nan\n",
    "    \n",
    "for f in hexadecimal_features:\n",
    "    features[f] = features[f].apply(lambda x: conv_hex(x))\n",
    "\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test set\n",
    "20% of the dataset is held off for testing our model. We make sure to only fit our models on the training data, so that there is no leakage of information from the test set into the model. We use a stratified split so that the class balance is maintained between the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_pre, X_test_pre, y_train, y_test = train_test_split(features, labels, test_size=0.2, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation, normalization and one-hot encoding\n",
    "We use a ColumnTransformer to distinguish between numeric and categorical features for preprocessing. While it is used to transform both training and test data, it is only fit on the training set.\n",
    "\n",
    "We use the IterativeImputer and standard scaling for normalization of numeric features. For categorical features, we use one-hot encoding while ignore missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>4.000000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "      <td>40000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.032863e-15</td>\n",
       "      <td>-3.002043e-16</td>\n",
       "      <td>1.278977e-16</td>\n",
       "      <td>2.483347e-16</td>\n",
       "      <td>-1.385558e-17</td>\n",
       "      <td>-2.749800e-16</td>\n",
       "      <td>2.899903e-17</td>\n",
       "      <td>-1.030287e-16</td>\n",
       "      <td>-1.177725e-16</td>\n",
       "      <td>-7.904788e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059375</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.029850</td>\n",
       "      <td>0.198150</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>0.174825</td>\n",
       "      <td>0.253875</td>\n",
       "      <td>0.272050</td>\n",
       "      <td>0.029950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>1.000013e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.096498</td>\n",
       "      <td>0.170176</td>\n",
       "      <td>0.398611</td>\n",
       "      <td>0.208182</td>\n",
       "      <td>0.158391</td>\n",
       "      <td>0.379822</td>\n",
       "      <td>0.435232</td>\n",
       "      <td>0.445021</td>\n",
       "      <td>0.170452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.090566e+00</td>\n",
       "      <td>-1.880162e+00</td>\n",
       "      <td>-1.723150e+00</td>\n",
       "      <td>-1.725214e+00</td>\n",
       "      <td>-1.738972e+00</td>\n",
       "      <td>-1.804295e+00</td>\n",
       "      <td>-1.680943e+00</td>\n",
       "      <td>-3.283373e-01</td>\n",
       "      <td>-1.091963e+00</td>\n",
       "      <td>-6.273604e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.587428e-01</td>\n",
       "      <td>-8.664793e-01</td>\n",
       "      <td>-8.255266e-01</td>\n",
       "      <td>-8.642991e-01</td>\n",
       "      <td>-8.192244e-01</td>\n",
       "      <td>-7.860456e-01</td>\n",
       "      <td>-7.902617e-01</td>\n",
       "      <td>-3.283373e-01</td>\n",
       "      <td>-1.091963e+00</td>\n",
       "      <td>-6.273604e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-9.400274e-04</td>\n",
       "      <td>5.505008e-02</td>\n",
       "      <td>-2.783397e-03</td>\n",
       "      <td>-1.033519e-04</td>\n",
       "      <td>-1.233428e-03</td>\n",
       "      <td>-3.919653e-02</td>\n",
       "      <td>-5.591908e-04</td>\n",
       "      <td>-3.283373e-01</td>\n",
       "      <td>9.440065e-01</td>\n",
       "      <td>-6.273604e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.311391e-01</td>\n",
       "      <td>9.151441e-01</td>\n",
       "      <td>8.391106e-01</td>\n",
       "      <td>8.287135e-01</td>\n",
       "      <td>8.186611e-01</td>\n",
       "      <td>8.550373e-01</td>\n",
       "      <td>6.914578e-01</td>\n",
       "      <td>-3.283373e-01</td>\n",
       "      <td>9.440065e-01</td>\n",
       "      <td>1.641422e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.255307e+00</td>\n",
       "      <td>1.283756e+00</td>\n",
       "      <td>1.802649e+00</td>\n",
       "      <td>1.768538e+00</td>\n",
       "      <td>1.812479e+00</td>\n",
       "      <td>1.792955e+00</td>\n",
       "      <td>2.049499e+00</td>\n",
       "      <td>3.141297e+00</td>\n",
       "      <td>9.440065e-01</td>\n",
       "      <td>1.641422e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 77 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  4.000000e+04  4.000000e+04  4.000000e+04  4.000000e+04  4.000000e+04   \n",
       "mean  -2.032863e-15 -3.002043e-16  1.278977e-16  2.483347e-16 -1.385558e-17   \n",
       "std    1.000013e+00  1.000013e+00  1.000013e+00  1.000013e+00  1.000013e+00   \n",
       "min   -3.090566e+00 -1.880162e+00 -1.723150e+00 -1.725214e+00 -1.738972e+00   \n",
       "25%   -7.587428e-01 -8.664793e-01 -8.255266e-01 -8.642991e-01 -8.192244e-01   \n",
       "50%   -9.400274e-04  5.505008e-02 -2.783397e-03 -1.033519e-04 -1.233428e-03   \n",
       "75%    7.311391e-01  9.151441e-01  8.391106e-01  8.287135e-01  8.186611e-01   \n",
       "max    3.255307e+00  1.283756e+00  1.802649e+00  1.768538e+00  1.812479e+00   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  4.000000e+04  4.000000e+04  4.000000e+04  4.000000e+04  4.000000e+04   \n",
       "mean  -2.749800e-16  2.899903e-17 -1.030287e-16 -1.177725e-16 -7.904788e-17   \n",
       "std    1.000013e+00  1.000013e+00  1.000013e+00  1.000013e+00  1.000013e+00   \n",
       "min   -1.804295e+00 -1.680943e+00 -3.283373e-01 -1.091963e+00 -6.273604e-01   \n",
       "25%   -7.860456e-01 -7.902617e-01 -3.283373e-01 -1.091963e+00 -6.273604e-01   \n",
       "50%   -3.919653e-02 -5.591908e-04 -3.283373e-01  9.440065e-01 -6.273604e-01   \n",
       "75%    8.550373e-01  6.914578e-01 -3.283373e-01  9.440065e-01  1.641422e+00   \n",
       "max    1.792955e+00  2.049499e+00  3.141297e+00  9.440065e-01  1.641422e+00   \n",
       "\n",
       "       ...            67            68            69            70  \\\n",
       "count  ...  40000.000000  40000.000000  40000.000000  40000.000000   \n",
       "mean   ...      0.059375      0.009400      0.029850      0.198150   \n",
       "std    ...      0.236328      0.096498      0.170176      0.398611   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 71            72            73            74            75  \\\n",
       "count  40000.000000  40000.000000  40000.000000  40000.000000  40000.000000   \n",
       "mean       0.045400      0.025750      0.174825      0.253875      0.272050   \n",
       "std        0.208182      0.158391      0.379822      0.435232      0.445021   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      1.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 76  \n",
       "count  40000.000000  \n",
       "mean       0.029950  \n",
       "std        0.170452  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 77 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "numeric_transformer = make_pipeline(\n",
    "    IterativeImputer(random_state=0),\n",
    "    StandardScaler()\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "    OneHotEncoder(handle_unknown='ignore')\n",
    ")\n",
    "    \n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features + hexadecimal_features + boolean_features + ordinal_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")\n",
    "\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train_pre, y_train))\n",
    "X_test  = pd.DataFrame(preprocessor.transform(X_test_pre))\n",
    "\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "## Evaluation\n",
    "Because there is no perfect metric to evaluate the performance of a model, and we don't know which aspect of performance is the most valuable in this case, we decided to look at both the ROC AUC score and the F1 score while we were experimenting with different models. The ROC AUC score summarizes the TPR (true positive rate) and FPR (false positive rate) at different discrimination thresholds, and is independent of class distribution (which in our case is unbalanced), so it will be the primary metric by which we evaluate performance. Furthermore, the F1 score represents an average of precision and recall.\n",
    "\n",
    "Seeing as we know nothing about the problem domain, but we have a minority of 1s in the target, we make the assumption that having a high recall of the 1 class is more important than a high recall of the 0 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \n",
    "    metrics = {\n",
    "        'AUC': roc_auc_score,\n",
    "        'F1': f1_score,\n",
    "    }\n",
    "\n",
    "    for metric, fun in metrics.items():\n",
    "        print(f\"{metric} = {fun(y_true, y_pred):.2f}\")\n",
    "\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling\n",
    "To deal with our imbalanced classes, we initially tried using the SMOTE oversampling technique. However, we obtained better results using the simpler RandomOverSampler. Since oversampling a validation set would result in overly optimistic results during cross-validation, which would be detrimental to optimal model selection, we use an Imbalanced-Learn pipeline as a drop-in replacement for the scikit-learn pipeline. This implementation makes sure that oversampling is only applied to the training folds, not the validation folds, when performing cross-validation.\n",
    "\n",
    "## Baseline attempt using logistic regression\n",
    "We achieve an AUC of 0.67 by using logistic regression with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.66\n",
      "F1 = 0.42\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.66      0.76      8165\n",
      "           1       0.31      0.66      0.42      1835\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.60      0.66      0.59     10000\n",
      "weighted avg       0.79      0.66      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = make_pipeline(\n",
    "    RandomOverSampler(random_state=0),\n",
    "    LogisticRegression(random_state=0, n_jobs=6, max_iter=1000),\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(y_test, lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with balanced class weights, no oversampling\n",
    "Instead of using oversampling, one can try to compensate for class imbalance by weighting the classes by their inverse proportions in the training set. This approach gave marginally better results in this case, and worse results in others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_balanced = LogisticRegression(random_state=0, n_jobs=6, max_iter=1000, class_weight='balanced')\n",
    "\n",
    "lr_balanced.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(y_test, lr_balanced.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search using logistic regression\n",
    "Let's try to search for a better model using some different C values. Our grid search probably benefits from using a stratified k-fold split to maintain class balance between folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 0.012742749857031334}\n",
      "AUC = 0.66\n",
      "F1 = 0.41\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.66      0.76      8165\n",
      "           1       0.30      0.66      0.41      1835\n",
      "\n",
      "    accuracy                           0.66     10000\n",
      "   macro avg       0.60      0.66      0.59     10000\n",
      "weighted avg       0.79      0.66      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "params = {\n",
    "    'logisticregression__C': np.logspace(-4, 4, 20)\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    lr,\n",
    "    param_grid=params,\n",
    "    cv=kf,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=6\n",
    ")\n",
    "\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "print(grid_lr.best_params_)\n",
    "\n",
    "evaluate_model(y_test, grid_lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search yields no better results than our baseline, so we move on to a different type of model.\n",
    "## Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = make_pipeline(\n",
    "    RandomOverSampler(random_state=0),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    ")\n",
    "\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(y_test, svc.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "We attempted to reduce the dimensionality of the feature space using PCA, although this only gave worse performance. This makes sense considering our features have a low degree of correlation already. Lower values of n_components gave worse results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.62\n",
      "F1 = 0.38\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74      8165\n",
      "           1       0.27      0.60      0.38      1835\n",
      "\n",
      "    accuracy                           0.63     10000\n",
      "   macro avg       0.57      0.62      0.56     10000\n",
      "weighted avg       0.77      0.63      0.67     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = make_pipeline(\n",
    "    RandomOverSampler(random_state=0),\n",
    "    PCA(n_components=30),\n",
    "    LogisticRegression(random_state=0, n_jobs=6, max_iter=1000),\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "pca.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(y_test, pca.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MML] *",
   "language": "python",
   "name": "conda-env-MML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
